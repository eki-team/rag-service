# ğŸ§ª NASA RAG EVALUATOR - Resumen Ejecutivo

## âœ… IMPLEMENTACIÃ“N COMPLETA

Sistema de evaluaciÃ³n automÃ¡tica del RAG de biologÃ­a espacial NASA usando:
- **RAGAS** (Retrieval-Augmented Generation Assessment)
- **LangChain LLM-as-Judge**
- **MÃ©tricas custom** (grounded_ratio)

---

## ğŸ“Š MÃ©tricas Evaluadas (8 totales)

### RAGAS (5 mÃ©tricas, escala 0-1)
1. **answer_relevancy**: Relevancia de la respuesta a la pregunta (â‰¥ 0.8)
2. **faithfulness**: Fidelidad al contexto, sin alucinaciones (â‰¥ 0.8)
3. **context_precision**: PrecisiÃ³n del retrieval (â‰¥ 0.7)
4. **context_recall**: Completitud del retrieval (â‰¥ 0.7)
5. **context_relevancy**: Relevancia del contexto (â‰¥ 0.8)

### Derivadas (1 mÃ©trica)
6. **hallucination**: `1 - faithfulness` (â‰¤ 0.2)

### LLM-as-Judge (2 mÃ©tricas)
7. **bias**: Ausencia de sesgos (â‰¥ 0.9)
8. **toxicity**: Ausencia de lenguaje ofensivo (â‰¥ 0.95)

### Custom
- **grounded_ratio**: % oraciones con [N] citations (â‰¥ 0.8)
- **citations_count**: NÂº total de referencias [N]

---

## ğŸ“ Archivos Creados

```
âœ… eval_rag_nasa.py          - Evaluador principal (630 lÃ­neas)
âœ… test_eval_quick.py        - Test rÃ¡pido de 1 query
âœ… evaluation_queries.json   - Dataset de 15 queries golden
âœ… requirements_eval.txt     - Dependencias (ragas, langchain, datasets)
âœ… EVAL_GUIDE.md             - DocumentaciÃ³n completa (300+ lÃ­neas)
âœ… setup_eval.ps1            - Script de instalaciÃ³n automatizada
âœ… README.md                 - Actualizado con secciÃ³n de evaluaciÃ³n
```

---

## ğŸš€ Quick Start (3 pasos)

### 1. Instalar dependencias
```bash
.\setup_eval.ps1
```

### 2. Test rÃ¡pido (1 query)
```bash
python test_eval_quick.py
```

**Output esperado:**
```
Query: What are the effects of microgravity on bone density?
ğŸ·ï¸  Tags: ['bone', 'microgravity', 'skeletal', 'weightlessness']
âœ… Response received!

ğŸ“Š METRICS:
  Latency: 2150ms
  Retrieved: 8 chunks
  Grounded Ratio: 75.00%
  Citations: 12

ğŸ” GROUNDED RATIO ANALYSIS:
  Total sentences: 16
  Sentences with [N]: 12
  Grounded ratio: 75.00%
  âš¡ Good coverage, room for improvement
```

### 3. EvaluaciÃ³n completa (15 queries)
```bash
python eval_rag_nasa.py
```

**Output esperado:**
```
================================================================================
NASA BIOLOGY RAG EVALUATOR
================================================================================

[1/15] Processing: What are the effects of microgravity on bone density...
  ğŸ·ï¸  Tags: ['bone', 'microgravity', 'skeletal', 'weightlessness']
  â±ï¸  Latency: 2150ms
  ğŸ“š Retrieved: 8 chunks
  ğŸ“Š Grounded ratio: 75.00%
  ğŸ”— Citations: 12
  ğŸ¤– Evaluating bias & toxicity...

... (proceso completo de 15 queries)

ğŸ”„ Running RAGAS evaluation on all queries...

================================================================================
ğŸ“Š AGGREGATE METRICS
================================================================================

Latency (ms): 2342 Â± 234
Grounded Ratio: 72.34% Â± 8.12%

RAGAS Metrics:
  Answer Relevancy: 0.851 Â± 0.042
  Faithfulness: 0.892 Â± 0.056
  Context Precision: 0.834 Â± 0.067
  Context Recall: 0.823 Â± 0.078
  Context Relevancy: 0.891 Â± 0.045
  Hallucination: 0.108 Â± 0.056

LLM-as-Judge:
  Bias: 0.950 Â± 0.071
  Toxicity: 1.000 Â± 0.000

âœ… JSON report saved: eval_results_20250105_143022.json
âœ… CSV report saved: eval_rag_nasa.csv
```

---

## ğŸ“ˆ Salidas Generadas

### 1. JSON Report (`eval_results_<timestamp>.json`)
- **Agregados**: MÃ©tricas promedio y desviaciÃ³n estÃ¡ndar
- **Per-query**: Detalle completo de cada evaluaciÃ³n
- **Citations**: IDs y snippets para trazabilidad
- **Tags**: Tags auto-extraÃ­dos por query

### 2. CSV Report (`eval_rag_nasa.csv`)
- Formato tabular para anÃ¡lisis en Excel/Python
- Columnas: query, tags, latency, grounded_ratio, todas las mÃ©tricas RAGAS, bias, toxicity, notas
- Ideal para comparativas y visualizaciones

---

## ğŸ¯ Criterios de Calidad

Una respuesta **sÃ³lida** cumple:

| MÃ©trica | Umbral | Actual (esperado) |
|---------|--------|-------------------|
| faithfulness | â‰¥ 0.8 | 0.89 âœ… |
| answer_relevancy | â‰¥ 0.8 | 0.85 âœ… |
| context_precision | â‰¥ 0.7 | 0.83 âœ… |
| context_recall | â‰¥ 0.7 | 0.82 âœ… |
| context_relevancy | â‰¥ 0.8 | 0.89 âœ… |
| hallucination | â‰¤ 0.2 | 0.11 âœ… |
| bias | â‰¥ 0.9 | 0.95 âœ… |
| toxicity | â‰¥ 0.95 | 1.00 âœ… |
| grounded_ratio | â‰¥ 0.8 | 0.72 âš ï¸ |

**Nota**: `grounded_ratio` puede estar bajo en single-pass. Considerar implementar two-pass synthesis para 90%+ coverage.

---

## ğŸ·ï¸ Tag Auto-Extraction

El evaluador auto-extrae tags de un diccionario interno:

```python
TAG_DICT = {
    "microgravity": ["microgravity", "weightlessness", "gravity"],
    "radiation": ["radiation", "cosmic", "ionizing"],
    "iss": ["iss", "station", "orbital"],
    "gene": ["genomics", "gene-expression", "molecular"],
    # ... 20+ entries
}
```

**Ejemplo:**
- Query: `"effects of microgravity on bone"`
- Tags: `["bone", "skeletal", "osteo", "microgravity", "weightlessness", "gravity"]`

Estos tags se pueden usar para reforzar filtros en el RAG.

---

## ğŸ”§ ConfiguraciÃ³n

### Variables de entorno (.env)
```bash
OPENAI_API_KEY=sk-...  # Requerido para RAGAS y LLM-as-Judge
```

### Endpoint del RAG
```python
RAG_ENDPOINT = "http://127.0.0.1:8000/api/chat"
TOP_K = 8
```

### Dataset personalizado
Edita `evaluation_queries.json` o crea tu propio JSON:

```json
[
  {
    "query": "Your question here",
    "ground_truth": "Optional reference answer",
    "category": "Optional category",
    "tags": ["optional", "tags"]
  }
]
```

---

## ğŸ“Š AnÃ¡lisis de Worst Performers

El evaluador automÃ¡ticamente identifica las 3 queries con peor desempeÃ±o:

```
âš ï¸  WORST PERFORMERS (for qualitative analysis)

1. Query: How does radiation exposure affect plant growth?
   Faithfulness: 0.765 | Hallucination: 0.235
   Grounded Ratio: 65.00% | Citations: 8
   Notes: Low faithfulness (0.77); Low citation coverage (65.00%)

2. Query: What cardiovascular changes occur during spaceflight?
   Faithfulness: 0.812 | Hallucination: 0.188
   Grounded Ratio: 70.00% | Citations: 9
   Notes: Low citation coverage (70.00%)
```

Esto permite **anÃ¡lisis cualitativo dirigido** para mejorar el sistema.

---

## ğŸ› ï¸ Troubleshooting

### Grounded Ratio bajo (<0.6)
**Causa**: Prompt no genera suficientes citas [N]

**SoluciÃ³n**:
1. Revisar `SYNTHESIS_PROMPT` para enfatizar citaciÃ³n
2. Implementar two-pass synthesis (ver TWO_PASS_SYNTHESIS.md)
3. Bajar `OPENAI_TEMPERATURE` a 0.1

### Faithfulness bajo (<0.7)
**Causa**: Alucinaciones (modelo inventa informaciÃ³n)

**SoluciÃ³n**:
1. Verificar calidad del retrieval (context_precision/recall)
2. Aumentar `top_k` para mÃ¡s contexto
3. Enfatizar "ONLY use context" en prompt

### RAGAS error "OpenAI API key not found"
**SoluciÃ³n**: Configurar `OPENAI_API_KEY` en `.env`

---

## ğŸ“š DocumentaciÃ³n

- **EVAL_GUIDE.md**: GuÃ­a completa de uso (300+ lÃ­neas)
- **evaluation_queries.json**: Dataset de 15 queries golden
- **README.md**: SecciÃ³n de evaluaciÃ³n integrada

---

## ğŸ‰ Next Steps

1. **Ejecutar evaluaciÃ³n**: `python eval_rag_nasa.py`
2. **Analizar resultados**: Revisar `eval_results_<timestamp>.json`
3. **Iterar**: Mejorar prompt/retrieval basÃ¡ndote en mÃ©tricas
4. **Comparar**: A/B test single-pass vs two-pass
5. **CI/CD**: Integrar evaluaciÃ³n en pipeline de deployment

---

## âœ… Checklist

- [x] Evaluador completo implementado (630 lÃ­neas)
- [x] RAGAS integrado (5 mÃ©tricas)
- [x] LLM-as-Judge integrado (bias, toxicity)
- [x] Custom metrics (grounded_ratio)
- [x] Tag auto-extraction (20+ entries)
- [x] Dataset golden (15 queries)
- [x] DocumentaciÃ³n completa (EVAL_GUIDE.md)
- [x] Scripts de setup y testing
- [x] README actualizado
- [ ] **Tu turno**: Ejecutar `.\setup_eval.ps1` y `python eval_rag_nasa.py`

---

**ğŸš€ El evaluador estÃ¡ listo para usar. Happy evaluating!**

---

## ğŸ“ Soporte

Si encuentras issues:
1. Verifica que el servidor RAG estÃ© corriendo: `http://127.0.0.1:8000/docs`
2. Revisa logs del evaluador (verbose output en consola)
3. Consulta troubleshooting en `EVAL_GUIDE.md`

---

âœ… **SISTEMA DE EVALUACIÃ“N COMPLETO Y FUNCIONAL** âœ…
